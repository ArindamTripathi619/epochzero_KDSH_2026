# Kharagpur Data Science Hackathon 2026 - Complete Project Overview

## ğŸ“‹ Executive Summary

This document provides a comprehensive overview of the **KDSH 2026 Hackathon** project, including the problem statement, provided resources, implementation approach, and how all components work together to solve the challenge.

---

## ğŸ¯ The Challenge: Narrative Consistency Classification

### What Are We Being Asked To Do?

The core task is a **binary classification problem** with a twist: instead of analyzing short texts, we must reason over **long-form narratives (100k+ word novels)** to determine if a hypothetical character backstory is consistent with the established story.

**Input:**
1. A complete novel (100,000+ words) - e.g., "The Count of Monte Cristo" (464,020 words) or "In Search of the Castaways" (138,830 words)
2. A hypothetical backstory for one of the novel's characters (newly written, not from the original text)

**Output:**
- **Binary Label:** 1 (Consistent) or 0 (Contradict)
- **Rationale:** (Optional for Track A) Evidence-based explanation referencing specific parts of the novel

### Why Is This Hard?

This is NOT about:
- âŒ Checking for direct textual contradictions
- âŒ Judging writing quality
- âŒ Simple keyword matching

This IS about:
- âœ… **Global consistency over time** - tracking how character traits, events, and constraints evolve throughout the narrative
- âœ… **Causal reasoning** - determining if later events make logical sense given the proposed backstory
- âœ… **Temporal constraint tracking** - ensuring timeline compatibility (childhood events â†’ adult outcomes)
- âœ… **Evidence aggregation** - drawing conclusions from multiple scattered passages, not a single quote

**Critical Principle:** "Silence â‰  Contradiction"
- If the novel never mentions a backstory element, but it plausibly fits, it's CONSISTENT
- Only explicit contradictions with established narrative facts count as CONTRADICT

---

## ğŸ“¦ What Has Been Provided

### 1. Datasets

#### Training Data (`train.csv`) - 81 rows
```csv
id, book_name, char, caption, content, label
```

**Example:**
```
id: 46
book_name: In Search of the Castaways
char: Thalcave
caption: [empty]
content: "Thalcave's people faded as colonists advanced; his father, last of the tribal guides, 
         knew the pampas geography and animal ways..."
label: consistent
```

**Label Distribution:**
- ~60% Consistent
- ~40% Contradict

**Characters Covered:**
- **In Search of the Castaways:** Thalcave, Kai-Koumou, Jacques Paganel, Tom Ayrton/Ben Joyce
- **The Count of Monte Cristo:** Faria, Noirtier

#### Test Data (`test.csv`) - 61 rows
- Same structure as training data
- **NO LABELS** (these are what we need to predict)

### 2. Novels (Full Text)

Located in `Dataset/Books/`:

1. **In search of the castaways.txt** - 138,830 words
   - Adventure novel by Jules Verne
   - Features characters like Thalcave (native guide), Jacques Paganel (geographer)

2. **The Count of Monte Cristo.txt** - 464,020 words
   - Classic by Alexandre Dumas
   - Features characters like AbbÃ© Faria (imprisoned priest), Noirtier (political figure)

**Format:** Plain text files with chapter divisions (e.g., "CHAPTER I", "Chapter 1")

### 3. Technology Stack Requirements

#### Mandatory for Track A:
- **Pathway Framework** - Must be used for at least one meaningful part of the pipeline:
  - Document ingestion and management
  - Vector store and indexing
  - Retrieval over long documents
  - Pipeline orchestration

#### Recommended/Open:
- Any transformer-based LLMs or Agentic pipelines
- Classical NLP pipelines
- Hybrid symbolic-neural approaches
- Custom classifiers, rerankers, or heuristics

### 4. Repository References

Two GitHub repositories are provided as references:

**pathwaycom/pathway** - Core streaming data framework
- Real-time data processing engine
- Built-in vector store capabilities
- Connector ecosystem (file systems, APIs, databases)

**pathwaycom/llm-app** - LLM application templates
- Live Document Indexing templates
- RAG (Retrieval-Augmented Generation) pipelines
- Question-Answering systems
- LangGraph agent cookbooks

---

## ğŸ—ï¸ Our Implementation Approach

### Architecture: Retrieval-Augmented Judge (RAG)

Our solution follows a **three-layer architecture**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    INPUT LAYER                              â”‚
â”‚  - train.csv / test.csv (backstory queries)                 â”‚
â”‚  - Novel .txt files (long-form narratives)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              PATHWAY ORCHESTRATION LAYER                    â”‚
â”‚                                                             â”‚
â”‚  1. Document Ingestion (retrieval.py)                      â”‚
â”‚     - Read novels from filesystem                          â”‚
â”‚     - Split by chapters using regex patterns               â”‚
â”‚     - Extract temporal metadata (chapter, progress %)      â”‚
â”‚     - Further chunk into 200-600 token segments            â”‚
â”‚                                                             â”‚
â”‚  2. Vector Indexing                                         â”‚
â”‚     - Embed chunks using SentenceTransformer               â”‚
â”‚       (all-MiniLM-L6-v2)                                   â”‚
â”‚     - Build vector store with BruteForceKnnFactory         â”‚
â”‚     - Preserve metadata (chapter, book, progress)          â”‚
â”‚                                                             â”‚
â”‚  3. Query Processing (main.py)                             â”‚
â”‚     - Read backstory queries from CSV                      â”‚
â”‚     - Build retrieval query: char + content                â”‚
â”‚     - Retrieve top-k (15) relevant chunks                  â”‚
â”‚     - Filter results by target book                        â”‚
â”‚     - Format evidence with temporal markers                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                REASONING LAYER                              â”‚
â”‚                                                             â”‚
â”‚  LLM Judge (llm_judge.py)                                  â”‚
â”‚     - Dual execution mode:                                  â”‚
â”‚       â€¢ Cloud: OpenRouter API (Claude, GPT-4, etc.)        â”‚
â”‚       â€¢ Local: Ollama (Mistral, Llama)                     â”‚
â”‚                                                             â”‚
â”‚     - Structured prompt with:                              â”‚
â”‚       â€¢ Character name                                      â”‚
â”‚       â€¢ Hypothetical backstory                             â”‚
â”‚       â€¢ Retrieved evidence (with chapters/progress)        â”‚
â”‚       â€¢ Task definition and reasoning guidelines           â”‚
â”‚                                                             â”‚
â”‚     - Output: JSON with label (0/1) + rationale           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    OUTPUT LAYER                             â”‚
â”‚  - results.csv (Story ID, Prediction, Rationale)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Component Breakdown

#### 1. **Document Ingestion & Chunking** (`retrieval.py`)

**Purpose:** Transform raw 100k+ word novels into searchable, indexed chunks while preserving narrative structure.

**Process:**
1. **Read novels** using Pathway's filesystem connector
2. **Chapter Detection** with regex patterns:
   ```python
   pattern = r'^(?:CHAPTER|Chapter|PART|Part|BOOK|Book)\s+(?:[IVXLCDM\d]+|[A-Z]+).*$'
   ```
3. **Temporal Metadata Extraction:**
   - Chapter title (e.g., "CHAPTER V")
   - Progress percentage (e.g., "14%" = 14% through the novel)
   - Source file path
4. **Token-level Chunking:** Further split chapters into 200-600 token chunks with overlap
5. **Embedding:** Convert each chunk to a vector using SentenceTransformer

**Why This Matters:**
- Chapter/progress metadata enables temporal reasoning ("Did X happen before Y?")
- Overlapping chunks prevent information loss at boundaries
- Filtering by book ensures we only retrieve relevant evidence

#### 2. **Retrieval System** (`NarrativeRetriever` class)

**Query Construction:**
```python
query = character + " " + backstory_content
# e.g., "Thalcave father tribal guide pampas geography animal ways"
```

**Retrieval Strategy:**
1. Embed the query using the same SentenceTransformer
2. Find top-15 most similar chunks via vector similarity (cosine)
3. Post-filter to ensure all chunks are from the target book
4. Format evidence with temporal markers:
   ```
   [CHAPTER V | 14%]
   The old guide, weathered by years on the pampas...
   ---
   [CHAPTER XII | 34%]
   Thalcave's knowledge of animal tracks proved invaluable...
   ```

**Why k=15?**
- Balance between context (more evidence) and noise
- Long novels require gathering evidence from multiple locations
- LLMs can handle 15 short chunks (~200-600 tokens each = 3000-9000 tokens total)

#### 3. **LLM Judge** (`llm_judge.py`)

**Core Prompt Structure:**
```
You are a senior literary editor and consistency judge.

INPUTS:
1. Character: [e.g., Thalcave]
2. Hypothetical Backstory: [proposed history]
3. Evidence Excerpts (with temporal metadata): [retrieved chunks]

ANALYSIS GUIDELINES:
- Temporal Consistency: Use chapter/progress tags to build timeline
- Causal Consistency: Check if motivations match actions
- Silence â‰  Contradiction: Absence of mention is NOT a contradiction

OUTPUT: JSON with "label" (0/1) and "rationale" (evidence-based explanation)
```

**Execution Modes:**

**Local Mode (Development):**
```python
judge = ConsistencyJudge(use_cloud=False, model_name="mistral")
# Uses Ollama running on localhost:11434
# Fast, free, but less accurate on subtle contradictions
```

**Cloud Mode (Production):**
```python
judge = ConsistencyJudge(use_cloud=True, model_name="anthropic/claude-3.5-sonnet")
# Uses OpenRouter API for access to SOTA models
# Slower, costs money, but superior reasoning
```

**Recommended Models:**
- **Best Accuracy:** `anthropic/claude-3.5-sonnet` (200k context, excellent at "needle in haystack")
- **Budget:** `deepseek/deepseek-chat` (128k context, near GPT-4 performance, very cheap)
- **Massive Context:** `google/gemini-pro-1.5` (2M context, can handle entire chapters)

#### 4. **Pipeline Orchestration** (`main.py`)

**Pathway's Role:**
```python
# 1. Define data schema
class QuerySchema(pw.Schema):
    id: int
    book_name: str
    char: str
    caption: str
    content: str

# 2. Read CSV as Pathway table
queries = pw.io.csv.read(INPUT_CSV, schema=QuerySchema, mode="static")

# 3. Apply transformations as table operations
query_table = queries.select(
    query=pw.this["char"] + " " + pw.this["content"],
    # ... other fields
)

# 4. Retrieval (returns table with 'retrieved_chunks' column)
retrieved = retriever.retrieve(query_table, k=15)

# 5. Evidence formatting (UDF applied to table)
evidence_table = retrieved.select(
    evidence=combine_evidence(pw.this.retrieved_chunks, ...)
)

# 6. LLM judgment (another table transformation)
results_table = judge.judge(evidence_table)

# 7. Export to CSV
pw.io.csv.write(results_table, OUTPUT_FILE)

# 8. Execute the entire compute graph
pw.run()
```

**Why Pathway?**
- **Declarative pipelines:** Define what to compute, not how
- **Type safety:** Schema validation prevents runtime errors
- **Scalability:** Can handle streaming data or batch processing
- **Built-in vector store:** No need for separate Pinecone/Weaviate
- **Hackathon requirement:** Track A explicitly requires meaningful Pathway usage

---

## ğŸ“Š How the Datasets Are Used

### Training Data (`train.csv`) - 81 Examples

**Purpose: Model Development & Validation**

1. **Prompt Engineering:**
   - Test different prompt formats
   - Calibrate "reasoning guidelines" (temporal consistency, causal logic)
   - Validate that LLM correctly interprets the task

2. **Retrieval Tuning:**
   - Determine optimal k (number of chunks to retrieve)
   - Test different query formulations (char + content vs. content alone)
   - Evaluate book filtering effectiveness

3. **Error Analysis:**
   - Identify failure modes (e.g., over-predicting "consistent")
   - Find which types of contradictions are hard to detect
   - Understand when retrieval misses key evidence

4. **Ground Truth for Evaluation:**
   - Compare system predictions against known labels
   - Calculate accuracy, precision, recall, F1
   - Ensure no overfitting to training patterns

**Example Training Case Study:**

```
ID: 137
Book: The Count of Monte Cristo
Character: Faria
Content: "Suspected again in 1815, he was re-arrested and shipped to the 
         ChÃ¢teau d'If, this time for life."
Label: contradict
```

**Why Contradict?**
The novel establishes that AbbÃ© Faria was already imprisoned at ChÃ¢teau d'If BEFORE 1815, not re-arrested in 1815. The backstory claims a second arrest, but the timeline shows continuous imprisonment.

**What This Teaches Us:**
- Need to retrieve passages about Faria's imprisonment timeline
- LLM must compare dates/sequences ("before 1815" vs "in 1815")
- Temporal reasoning is critical

### Test Data (`test.csv`) - 61 Examples

**Purpose: Final Submission**

- These are the examples we must predict for the competition
- No labels provided (we discover accuracy only after submission)
- Represents "unseen" data our system hasn't been tuned on
- Output format: `Story ID, Prediction, Rationale`

**Processing Flow:**
```python
# Same pipeline as training, just without labels
test_queries = pw.io.csv.read("test.csv", schema=QuerySchema)
â†’ Retrieval (top-15 chunks from correct book)
â†’ LLM Judge (predict 0 or 1 + rationale)
â†’ Export to results.csv
```

### How the Novels Are Used

#### "In Search of the Castaways" (138,830 words)

**Novel Context:** Jules Verne adventure about searching for a shipwrecked captain.

**Characters in Dataset:**
- **Thalcave** - Native guide, expert tracker
- **Kai-Koumou** - Maori warrior chief
- **Jacques Paganel** - French geographer (absent-minded)
- **Tom Ayrton/Ben Joyce** - Mutineer

**Example Retrieval Scenario:**

*Backstory Query:* "Thalcave's father was the last of the tribal guides..."

*Retrieved Evidence:*
```
[CHAPTER III | 8%]
The party had hired an experienced guide, Thalcave, whose knowledge 
of the pampas was unmatched. His skill with horses...

[CHAPTER XIV | 42%]
Thalcave spoke rarely of his past, but when asked, he mentioned his 
father taught him to read animal tracks as a child...
```

*Judgment:* CONSISTENT (backstory about father teaching tracking aligns with novel's depiction)

#### "The Count of Monte Cristo" (464,020 words)

**Novel Context:** Classic tale of betrayal, imprisonment, and revenge.

**Characters in Dataset:**
- **AbbÃ© Faria** - Fellow prisoner who mentors Edmond DantÃ¨s
- **Noirtier** - Villefort's father, former Bonapartist

**Example Retrieval Scenario:**

*Backstory Query:* "Noirtier met the Count of Monte Cristo through underground circles and fed him information..."

*Retrieved Evidence:*
```
[CHAPTER XXIV | 12%]
Noirtier de Villefort, the grandfather, remained in the background, a 
silent figure whose past was shrouded in revolutionary intrigue...

[CHAPTER L | 34%]
The old man [Noirtier] had no knowledge of the mysterious Count who now 
frequented his son's house. His paralyzed state prevented any direct 
communication...
```

*Judgment:* CONTRADICT (Noirtier never interacts with or aids the Count; they exist in separate spheres)

---

## ğŸ”¬ Evaluation Criteria (Track A)

Our submission will be judged on:

### 1. Accuracy & Robustness (Primary Metric)
- Binary classification accuracy on test.csv
- Generalization across different types of contradictions
- **Current Status:** Needs improvement (over-predicting "consistent")

### 2. Novelty (20% weight)
- **Our Innovations:**
  - âœ… Temporal metadata extraction (chapter + progress %)
  - âœ… Dual LLM execution (cloud vs. local)
  - âœ… Book-specific filtering post-retrieval
  - âœ… Structured evidence formatting with timeline markers

### 3. Long Context Handling (30% weight)
- **Our Approach:**
  - âœ… Chapter-aware chunking preserves narrative structure
  - âœ… Token-level splitting (200-600 tokens) with overlap
  - âœ… Vector indexing enables fast retrieval across 100k+ words
  - âœ… Metadata propagation maintains source tracking

### 4. Evidence-Based Reasoning (20% weight)
- **Our Strategy:**
  - âœ… Prompt explicitly requests chapter/quote citations
  - âœ… Rationale format: `EVIDENCE: [Chapter X] "..." â†’ CLAIM: ... â†’ ANALYSIS: ...`
  - âœ… Multiple retrieval chunks provide diverse evidence angles

---

## ğŸš€ Execution Workflow

### Development Phase (Current)

```bash
# 1. Environment Setup
conda create -n kdsh python=3.11
conda activate kdsh
pip install -r requirements.txt

# 2. Local LLM Testing (Free, Fast)
ollama pull mistral
python main.py  # Uses Ollama by default

# 3. Validation on Training Data
INPUT_DATA=Dataset/train.csv python main.py
# Compare results.csv predictions vs. ground truth labels
```

### Production Phase (Submission)

```bash
# 1. Switch to Cloud LLM
export OPENROUTER_API_KEY="your-key-here"

# 2. Update llm_judge.py
judge = ConsistencyJudge(use_cloud=True, model_name="anthropic/claude-3.5-sonnet")

# 3. Generate Test Predictions
INPUT_DATA=Dataset/test.csv python main.py
# Output: results.csv with 61 predictions

# 4. Submit results.csv to hackathon portal
```

---

## ğŸ“ˆ Current System Performance

### Strengths âœ…

1. **Clean Architecture:** Modular, well-separated concerns
2. **Pathway Integration:** Full compliance with Track A requirements
3. **Dual LLM Support:** Flexible cloud/local execution
4. **Temporal Reasoning:** Chapter/progress metadata enables timeline analysis
5. **Error Handling:** Robust JSON parsing, fallback responses

### Identified Issues âš ï¸

1. **Over-Prediction of "Consistent":**
   - ~95% of predictions are label=1
   - Likely due to insufficient evidence retrieval
   - LLM defaults to "silence â‰  contradiction" principle

2. **Retrieval Coverage:**
   - Current k=15 may still miss scattered evidence
   - Book filtering might be too strict (case sensitivity fixed)
   - Query construction could be enhanced (e.g., include caption field)

3. **Prompt Calibration:**
   - May need more explicit examples of contradictions
   - Could benefit from chain-of-thought prompting
   - Rationale format needs enforcement

### Improvement Roadmap ğŸ¯

**Phase 1: Retrieval Enhancement**
- [ ] Increase k to 20-25 for broader evidence coverage
- [ ] Experiment with query construction (add caption, rephrase content)
- [ ] Implement re-ranking (e.g., cross-encoder after initial retrieval)

**Phase 2: Prompt Engineering**
- [ ] Add 2-3 shot examples (consistent + contradict cases)
- [ ] Enforce stricter JSON output format
- [ ] Request explicit timeline construction ("Event A at X% â†’ Event B at Y%")

**Phase 3: Model Upgrade**
- [ ] Switch from local Mistral to Claude-3.5-Sonnet (OpenRouter)
- [ ] Test ensemble: multiple models vote on label
- [ ] Calibrate confidence thresholds

**Phase 4: Evidence Quality**
- [ ] Implement semantic de-duplication of retrieved chunks
- [ ] Add diversity penalty (avoid retrieving 15 similar passages)
- [ ] Experiment with chapter-level vs. chunk-level retrieval

---

## ğŸ§® Resource Utilization

### Hardware Profile (Ryzen 7 8840HS + 32GB RAM)

| Component | Resource Usage | Optimization |
|-----------|---------------|--------------|
| **Document Ingestion** | ~2 GB RAM | Efficient with Pathway streaming |
| **Vector Indexing** | ~4 GB RAM | In-memory index for 2 novels |
| **Embeddings** | CPU: 60-70% | SentenceTransformer on 16 cores |
| **Retrieval** | <1s per query | Brute-force KNN acceptable for small corpus |
| **LLM (Local Ollama)** | 8-16 GB RAM | Mistral 7B runs smoothly |
| **LLM (Cloud API)** | Network only | Offloads compute, costs ~$0.01-0.05/query |

**Recommended Split:**
- **Use Local:** Ingestion, indexing, embeddings, retrieval (free, fast)
- **Use Cloud:** LLM judgment for final submission (best accuracy)

---

## ğŸ“š Key Insights & Lessons

### 1. Long-Context â‰  Long-Input
- We don't send entire 464k-word novel to the LLM
- Vector retrieval extracts relevant 3k-9k token "needles" from the "haystack"
- This is why RAG architecture is critical

### 2. Temporal Metadata is Crucial
- "Childhood" event in backstory must occur before "adulthood" events in novel
- Chapter numbers + progress % enable timeline reasoning
- Without this, causal contradictions are invisible

### 3. Silence â‰  Contradiction is a Double-Edged Sword
- Correct principle: absence of evidence isn't evidence of contradiction
- Danger: LLM may over-apply this and refuse to mark contradictions
- Solution: Provide explicit examples of what DOES count as contradiction

### 4. Pathway's Value Proposition
- Declarative syntax reduces boilerplate (compare to raw PyTorch/TF pipelines)
- Built-in vector store eliminates need for separate databases
- Type safety catches errors at "compile time" (before pw.run())

### 5. Hackathon Strategy
- **Track A was the right choice:** Engineering-focused, uses existing tools
- **Track B would require:** BDH model pretraining, representation analysis (research-heavy)
- **Hybrid approach possible:** Use BDH embeddings in retriever (future work)

---

## ğŸ“ Submission Checklist

### Code Requirements âœ…
- [x] Pathway used for document ingestion
- [x] Pathway used for vector indexing
- [x] Pathway used for retrieval
- [x] Pathway used for pipeline orchestration
- [x] Clean, modular code structure
- [x] Error handling and logging

### Output Requirements âœ…
- [x] results.csv with correct schema (Story ID, Prediction, Rationale)
- [x] Binary predictions (0/1 only)
- [x] Evidence-based rationales (optional but recommended)

### Documentation Requirements (In Progress)
- [x] README.md with setup instructions
- [x] requirements.txt with all dependencies
- [ ] 10-page technical report explaining:
  - Long-context handling strategy
  - Novel architectural decisions
  - Failure mode analysis
  - Track A compliance details

### Repository Requirements
- [x] Code in `src/` directory
- [x] Data in `Dataset/` directory
- [x] Results in root directory
- [x] Clean git history (optional but professional)

---

## ğŸ”— References & Resources

### Official Documentation
- **Pathway Docs:** https://pathway.com/developers/
- **LLM-App Templates:** https://github.com/pathwaycom/llm-app
- **OpenRouter API:** https://openrouter.ai/docs

### Related Research
- **Long-Context Reasoning:** "Lost in the Middle" (Liu et al., 2023)
- **RAG Systems:** "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks" (Lewis et al., 2020)
- **Narrative Consistency:** "Probing Neural Language Models for Human Tacit Assumptions" (Forbes et al., 2019)

### Useful Tools
- **Ollama:** https://ollama.com (local LLM serving)
- **SentenceTransformers:** https://www.sbert.net (embedding models)
- **Pathway:** https://github.com/pathwaycom/pathway (core framework)

---

## ğŸ‘¥ Team Information

- **Team Name:** [Your Team Name]
- **Competition:** Kharagpur Data Science Hackathon 2026
- **Track:** A (Systems Reasoning with NLP and Generative AI)
- **Repository:** epochzero_KDSH_2026

---

## ğŸ“… Timeline & Milestones

- **âœ… Jan 5:** Initial setup, baseline retrieval pipeline
- **âœ… Jan 6:** LLM integration, first predictions generated
- **âœ… Jan 7:** Comprehensive documentation, error analysis
- **ğŸ¯ Jan 8-9:** Retrieval tuning, prompt optimization
- **ğŸ¯ Jan 10-11:** Cloud LLM testing, final predictions
- **ğŸ¯ Jan 12:** Report writing, submission preparation
- **ğŸš€ Jan 13:** Final submission deadline

---

## ğŸ“ Conclusion

This hackathon challenge tests a critical capability for next-generation AI systems: **reasoning over long contexts with temporal and causal awareness**. Unlike simple Q&A or summarization tasks, this requires:

1. **Evidence aggregation** across 100k+ word narratives
2. **Timeline reconstruction** from scattered mentions
3. **Causal logic** to determine if past â†’ present is coherent
4. **Distinguishing plausible from contradictory** (not just "mentioned vs. not mentioned")

Our RAG-based approach with Pathway orchestration provides a strong foundation. The key to success lies in:
- **Retrieval quality:** Getting the right evidence to the LLM
- **Prompt design:** Teaching the LLM to reason temporally and causally
- **Model selection:** Using SOTA models for nuanced judgment

This project showcases how modern data engineering (Pathway), vector search, and large language models combine to tackle problems that were impossible just a few years ago.

---

**Document Version:** 1.0  
**Last Updated:** January 7, 2026  
**Author:** DevCrewX Team  
**Status:** Living Document (will be updated as project evolves)
